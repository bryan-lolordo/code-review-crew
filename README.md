# Code Review Crew üîç

> **Multi-Agent AI Code Review System with Autonomous Fixing**

An intelligent code review system that combines AutoGen's multi-agent collaboration with LangGraph's iterative fixing workflows. Get production-ready code reviews from specialized AI agents, then watch as issues are automatically fixed using a hybrid pattern-matching + LLM approach.

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![AutoGen](https://img.shields.io/badge/AutoGen-multi--agent-green.svg)](https://github.com/microsoft/autogen)
[![LangGraph](https://img.shields.io/badge/LangGraph-workflow-orange.svg)](https://github.com/langchain-ai/langgraph)
[![Streamlit](https://img.shields.io/badge/Streamlit-UI-red.svg)](https://streamlit.io/)

---

## üåü Features

### ü§ñ Multi-Agent Code Review
- **6 Specialized AI Agents** work together to review your code
- **CodeAnalyzer** - Identifies code smells, anti-patterns, PEP 8 violations
- **SecurityReviewer** - Detects SQL injection, XSS, weak crypto, hardcoded secrets
- **PerformanceOptimizer** - Analyzes complexity, finds bottlenecks, suggests optimizations
- **TestGenerator** - Recommends comprehensive test cases
- **ReviewOrchestrator** - Coordinates the review workflow and synthesizes feedback
- **CodeExecutor** - Safely executes code in Docker sandbox for validation

### üî• Autonomous Code Fixing
- **Hybrid Fixing Approach**: Pattern-based fixes (fast, free) + LLM fallback (smart, adaptive)
- **Iterative Workflow**: Fixes issues one-by-one with testing after each change
- **LangGraph State Machine**: Transparent, debuggable fixing process
- **Real-time Progress**: See each iteration, pattern match, and fix applied

### üìä Interactive Web Interface
- **Streamlit UI**: Clean, intuitive interface for code submission and results
- **Code Comparison View**: Side-by-side original vs. fixed code
- **Process Logs**: Full visibility into agent reasoning and fixing workflow
- **Conversation History**: See how agents collaborate and make decisions

---

## üöÄ Demo Flow

Below are screenshots showing the review and auto-fix process:

   ![Submit Code Sample](assets/1.png)

   ![AutoGen Multi-Agent Collaboration](assets/2.png)

   ![Detailed Code Review Report](assets/3.png)

   ![LangGraph Iterative Fixing](assets/4.png)

   ![Before/After Transformation](assets/5.png)

---

## üöÄ Quick Start

### Prerequisites
```bash
# Python 3.9 or higher
python --version

# OpenAI API key
export OPENAI_API_KEY="sk-..."
```

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/code-review-crew.git
cd code-review-crew

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY
```

### Run the Application

```bash
# Start Streamlit app
streamlit run app.py
```

Navigate to `http://localhost:8501` in your browser.

---

## üìñ Usage

### Review Only Mode
Perfect for getting feedback without making changes:

```python
from unified_analyzer import UnifiedCodeAnalyzer

analyzer = UnifiedCodeAnalyzer()
results = analyzer.review_only(your_code)

# Results include:
# - Agent feedback
# - Issue severity rankings
# - Test recommendations
# - Action items
```

### Review + Auto-Fix Mode
Get reviews AND automatic fixes:

```python
analyzer = UnifiedCodeAnalyzer()
results = analyzer.review_and_fix(
    code=your_code,
    max_iterations=10
)

print(results['fixed_code'])
print(f"Fixed {results['issues_fixed']} issues")
```

### Example: Fixing Security Vulnerabilities

**Input Code:**
```python
def get_user(username):
    query = f"SELECT * FROM users WHERE name = '{username}'"
    return db.execute(query)

def hash_password(password):
    import hashlib
    return hashlib.md5(password.encode()).hexdigest()

API_KEY = "sk-1234567890abcdef"
```

**Issues Found:**
- ‚ùå SQL Injection vulnerability (Critical)
- ‚ùå Weak MD5 cryptography (Critical)
- ‚ùå Hardcoded API key (High)
- ‚ùå Import inside function (Medium)

**Fixed Code:**
```python
import hashlib
import os

# Fixed: SQL injection vulnerability
def get_user(username):
    query = "SELECT * FROM users WHERE name = ?"
    return db.execute(query, (username,))

# Fixed: Replaced MD5 with SHA256
def hash_password(password):
    return hashlib.sha256(password.encode()).hexdigest()

# Fixed: Moved secrets to environment variables
API_KEY = os.getenv("API_KEY")
```

---

## üèóÔ∏è Architecture

The system uses a **two-stage pipeline**:

### Stage 1: AutoGen Multi-Agent Review
```
User Code ‚Üí ReviewOrchestrator ‚Üí CodeAnalyzer
                                ‚Üí SecurityReviewer
                                ‚Üí PerformanceOptimizer
                                ‚Üí TestGenerator
                                ‚Üí Final Report
```

### Stage 2: LangGraph Iterative Fixing
```
Issues ‚Üí [Fix Issue ‚Üí Test Code ‚Üí Route] ‚Üí Fixed Code
            ‚Üë                         ‚Üì
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Continue ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

For detailed architecture, see [ARCHITECTURE.md](ARCHITECTURE.md).

---

## üéØ Supported Issue Types

### Pattern-Based Fixes (Fast, Free, Deterministic)
‚úÖ SQL Injection ‚Üí Parameterized queries  
‚úÖ Weak Crypto (MD5/SHA1) ‚Üí SHA256  
‚úÖ Hardcoded Secrets ‚Üí Environment variables  
‚úÖ Imports in Functions ‚Üí Move to top  
‚úÖ Nested Loops ‚Üí Optimization suggestions  

### LLM-Based Fixes (Smart, Adaptive)
ü§ñ Race Conditions ‚Üí Thread safety (locks, context managers)  
ü§ñ Memory Leaks ‚Üí Data structure optimization  
ü§ñ Pickle Vulnerabilities ‚Üí Safe serialization (JSON)  
ü§ñ Type Coercion Bugs ‚Üí Proper type conversion  
ü§ñ Input Validation ‚Üí Error handling and checks  
ü§ñ Division by Zero ‚Üí Boundary condition handling  
ü§ñ PCI Compliance ‚Üí Sensitive data masking  

---

## üîß Configuration

### Environment Variables
```bash
# Required
OPENAI_API_KEY=sk-...

# Optional
ANTHROPIC_API_KEY=...  # For Claude models
```

### LLM Configuration
```python
# In code_fixer/__init__.py
llm_config = {
    "model": "gpt-4",           # or "gpt-4-turbo", "gpt-3.5-turbo"
    "temperature": 0,           # 0 for deterministic, 0.7 for creative
    "api_key": os.getenv("OPENAI_API_KEY")
}
```

### Max Iterations
```python
# Higher = more thorough, but slower and more expensive
results = analyzer.review_and_fix(code, max_iterations=20)
```

---

## üìä Performance

### Speed
- **Pattern-Based Fix**: ~0.1s per issue
- **LLM-Based Fix**: ~2-5s per issue
- **Full Review + Fix (10 issues)**: ~30-60s

### Cost (GPT-4)
- **Review Only**: ~$0.05-0.10 per review
- **Auto-Fix (pattern-based)**: $0 additional
- **Auto-Fix (LLM fallback)**: ~$0.01-0.03 per fix

### Accuracy
- **Pattern-Based Fixes**: 100% success rate for matched patterns
- **LLM-Based Fixes**: ~85% success rate on first attempt
- **Overall Fix Rate**: ~90% of issues fixed automatically

---

## üß™ Testing

Run the test suite:
```bash
# Unit tests
pytest tests/

# Integration tests
pytest tests/integration/

# End-to-end tests
pytest tests/e2e/
```

Test with example code:
```bash
python -m code_review_crew.examples.test_all_examples
```

---

## üõ†Ô∏è Development

### Project Structure
```
code-review-crew/
‚îú‚îÄ‚îÄ app.py                          # Streamlit web interface
‚îú‚îÄ‚îÄ unified_analyzer.py             # Main orchestrator
‚îú‚îÄ‚îÄ log_capture.py                  # Console output capture
‚îú‚îÄ‚îÄ run_group_chat.py              # AutoGen group chat runner
‚îú‚îÄ‚îÄ code_review_crew/
‚îÇ   ‚îú‚îÄ‚îÄ agents/                    # AutoGen agents
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_agent.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ code_analyzer.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ security_reviewer.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ performance_optimizer.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_generator.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ orchestrator.py
‚îÇ   ‚îú‚îÄ‚îÄ tools/                     # Analysis tools
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ linting_tool.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ complexity_analyzer.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ security_scanner.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_runner.py
‚îÇ   ‚îî‚îÄ‚îÄ code_fixer/               # LangGraph fixer
‚îÇ       ‚îú‚îÄ‚îÄ fixer.py              # Workflow orchestrator
‚îÇ       ‚îú‚îÄ‚îÄ nodes.py              # Node functions
‚îÇ       ‚îú‚îÄ‚îÄ state.py              # State definitions
‚îÇ       ‚îî‚îÄ‚îÄ conditions.py         # Routing logic
‚îî‚îÄ‚îÄ tests/                        # Test suite
```

### Adding New Pattern Fixes
```python
# In code_fixer/nodes.py

def _generate_fix(self, code: str, issue: Dict) -> str:
    description = issue.get('description', '').lower()
    
    # Add your pattern
    if 'your_pattern' in description:
        return self._fix_your_issue(code)
    
    # ... existing patterns ...
```

### Adding New Agents
```python
# Create new agent in code_review_crew/agents/
from .base_agent import BaseAgent

class YourAgent(BaseAgent):
    def __init__(self, llm_config: Dict, tools: Dict):
        # Initialize your agent
        
    def create_agent(self):
        # Return AutoGen agent
        
    def register_functions(self):
        # Register tool functions
```

---

## ü§ù Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Guidelines
- Follow PEP 8 style guide
- Add tests for new features
- Update documentation
- Keep commits atomic and well-described

---

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **AutoGen** - Microsoft's multi-agent framework
- **LangGraph** - LangChain's state machine library
- **Streamlit** - Beautiful web interfaces for ML/AI apps
- **OpenAI** - GPT-4 for intelligent code analysis


---

## üó∫Ô∏è Roadmap

- [ ] Support for more programming languages (JavaScript, Java, C++)
- [ ] Integration with GitHub Actions for PR reviews
- [ ] VS Code extension
- [ ] Custom pattern definition UI
- [ ] Multi-file project analysis
- [ ] Code diff review (only review changed lines)
- [ ] Team collaboration features
- [ ] Configurable agent personas
- [ ] Performance benchmarking dashboard
